{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prakashksarangi/Python-Project-Code/blob/main/NLP_Course_End.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python Code of the Main LLM Model"
      ],
      "metadata": {
        "id": "0bwOlKhg9Rff"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNaPG_Th9J8X"
      },
      "outputs": [],
      "source": [
        "# !/usr/bin/env python3\n",
        "from dotenv import load_dotenv\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms import GPT4All, LlamaCpp\n",
        "import chromadb\n",
        "import os\n",
        "import argparse\n",
        "import time\n",
        "\n",
        "if not load_dotenv():\n",
        "    print(\"Could not load .env file or it is empty. Please check if it exists and is readable.\")\n",
        "    exit(1)\n",
        "\n",
        "embeddings_model_name = os.environ.get(\"EMBEDDINGS_MODEL_NAME\")\n",
        "persist_directory = os.environ.get('PERSIST_DIRECTORY')\n",
        "\n",
        "model_type = os.environ.get('MODEL_TYPE')\n",
        "model_path = os.environ.get('MODEL_PATH')\n",
        "model_n_ctx = os.environ.get('MODEL_N_CTX')\n",
        "model_n_batch = int(os.environ.get('MODEL_N_BATCH',8))\n",
        "target_source_chunks = int(os.environ.get('TARGET_SOURCE_CHUNKS',4))\n",
        "\n",
        "from constants import CHROMA_SETTINGS\n",
        "\n",
        "def main():\n",
        "    # n_gpu_layers = os.environ.get('N_GPU_LAYERS')\n",
        "    # # Added custom directory path for CUDA dynamic library\n",
        "    # os.add_dll_directory(\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.8/bin\")\n",
        "    # os.add_dll_directory(\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.8/extras/CUPTI/lib64\")\n",
        "    # os.add_dll_directory(\"C:/Program Files/NVIDIA GPU Computing Toolkit/CUDA/v11.8/include\")\n",
        "    # os.add_dll_directory(\"C:/tools/cuda/bin\")\n",
        "    # Parse the command line arguments\n",
        "    args = parse_arguments()\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
        "    chroma_client = chromadb.PersistentClient(settings=CHROMA_SETTINGS , path=persist_directory)\n",
        "    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings, client_settings=CHROMA_SETTINGS, client=chroma_client)\n",
        "    retriever = db.as_retriever(search_kwargs={\"k\": target_source_chunks})\n",
        "    # activate/deactivate the streaming StdOut callback for LLMs\n",
        "    callbacks = [] if args.mute_stream else [StreamingStdOutCallbackHandler()]\n",
        "    # Prepare the LLM\n",
        "    match model_type:\n",
        "        case \"LlamaCpp\":\n",
        "            llm = LlamaCpp(model_path=model_path, max_tokens=model_n_ctx, n_batch=model_n_batch, callbacks=callbacks, verbose=False, n_threads = 24)\n",
        "        case \"GPT4All\":\n",
        "            llm = GPT4All(model=model_path, max_tokens=model_n_ctx, backend='gptj', n_batch=model_n_batch, callbacks=callbacks, verbose=False)\n",
        "        case _default:\n",
        "            # raise exception if model_type is not supported\n",
        "            raise Exception(f\"Model type {model_type} is not supported. Please choose one of the following: LlamaCpp, GPT4All\")\n",
        "\n",
        "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents= not args.hide_source)\n",
        "    # Interactive questions and answers\n",
        "    while True:\n",
        "        query = input(\"\\nEnter a query: \")\n",
        "        if query == \"exit\":\n",
        "            break\n",
        "        if query.strip() == \"\":\n",
        "            continue\n",
        "\n",
        "        # Get the answer from the chain\n",
        "        start = time.time()\n",
        "        res = qa(query)\n",
        "        answer, docs = res['result'], [] if args.hide_source else res['source_documents']\n",
        "        end = time.time()\n",
        "\n",
        "        # Print the result\n",
        "        print(\"\\n\\n> Question:\")\n",
        "        print(query)\n",
        "        print(f\"\\n> Answer (took {round(end - start, 2)} s.):\")\n",
        "        print(answer)\n",
        "\n",
        "        # Print the relevant sources used for the answer\n",
        "        for document in docs:\n",
        "            print(\"\\n> \" + document.metadata[\"source\"] + \":\")\n",
        "            print(document.page_content)\n",
        "\n",
        "def parse_arguments():\n",
        "    parser = argparse.ArgumentParser(description='privateGPT: Ask questions to your documents without an internet connection, '\n",
        "                                                 'using the power of LLMs.')\n",
        "    parser.add_argument(\"--hide-source\", \"-S\", action='store_true',\n",
        "                        help='Use this flag to disable printing of source documents used for answers.')\n",
        "\n",
        "    parser.add_argument(\"--mute-stream\", \"-M\",\n",
        "                        action='store_true',\n",
        "                        help='Use this flag to disable the streaming StdOut callback for LLMs.')\n",
        "\n",
        "    return parser.parse_args()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Python Code for the Ingestion of Data into the LLM"
      ],
      "metadata": {
        "id": "GK9bfJEt9Z0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "import os\n",
        "import glob\n",
        "from typing import List\n",
        "from dotenv import load_dotenv\n",
        "from multiprocessing import Pool\n",
        "from tqdm import tqdm\n",
        "\n",
        "from langchain.document_loaders import (\n",
        "    CSVLoader,\n",
        "    EverNoteLoader,\n",
        "    PyMuPDFLoader,\n",
        "    TextLoader,\n",
        "    UnstructuredEmailLoader,\n",
        "    UnstructuredEPubLoader,\n",
        "    UnstructuredHTMLLoader,\n",
        "    UnstructuredMarkdownLoader,\n",
        "    UnstructuredODTLoader,\n",
        "    UnstructuredPowerPointLoader,\n",
        "    UnstructuredWordDocumentLoader,\n",
        ")\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "if not load_dotenv():\n",
        "    print(\"Could not load .env file or it is empty. Please check if it exists and is readable.\")\n",
        "    exit(1)\n",
        "\n",
        "from constants import CHROMA_SETTINGS\n",
        "import chromadb\n",
        "\n",
        "#Â Load environment variables\n",
        "persist_directory = os.environ.get('PERSIST_DIRECTORY')\n",
        "source_directory = os.environ.get('SOURCE_DIRECTORY', 'source_documents')\n",
        "embeddings_model_name = os.environ.get('EMBEDDINGS_MODEL_NAME')\n",
        "chunk_size = 500\n",
        "chunk_overlap = 50\n",
        "\n",
        "\n",
        "# Custom document loaders\n",
        "class MyElmLoader(UnstructuredEmailLoader):\n",
        "    \"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\n",
        "\n",
        "    def load(self) -> List[Document]:\n",
        "        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n",
        "        try:\n",
        "            try:\n",
        "                doc = UnstructuredEmailLoader.load(self)\n",
        "            except ValueError as e:\n",
        "                if 'text/html content not found in email' in str(e):\n",
        "                    # Try plain text\n",
        "                    self.unstructured_kwargs[\"content_source\"]=\"text/plain\"\n",
        "                    doc = UnstructuredEmailLoader.load(self)\n",
        "                else:\n",
        "                    raise\n",
        "        except Exception as e:\n",
        "            # Add file_path to exception message\n",
        "            raise type(e)(f\"{self.file_path}: {e}\") from e\n",
        "\n",
        "        return doc\n",
        "\n",
        "\n",
        "# Map file extensions to document loaders and their arguments\n",
        "LOADER_MAPPING = {\n",
        "    \".csv\": (CSVLoader, {}),\n",
        "    # \".docx\": (Docx2txtLoader, {}),\n",
        "    \".doc\": (UnstructuredWordDocumentLoader, {}),\n",
        "    \".docx\": (UnstructuredWordDocumentLoader, {}),\n",
        "    \".enex\": (EverNoteLoader, {}),\n",
        "    \".eml\": (MyElmLoader, {}),\n",
        "    \".epub\": (UnstructuredEPubLoader, {}),\n",
        "    \".html\": (UnstructuredHTMLLoader, {}),\n",
        "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
        "    \".odt\": (UnstructuredODTLoader, {}),\n",
        "    \".pdf\": (PyMuPDFLoader, {}),\n",
        "    \".ppt\": (UnstructuredPowerPointLoader, {}),\n",
        "    \".pptx\": (UnstructuredPowerPointLoader, {}),\n",
        "    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n",
        "    # Add more mappings for other file extensions and loaders as needed\n",
        "}\n",
        "\n",
        "\n",
        "def load_single_document(file_path: str) -> List[Document]:\n",
        "    ext = \".\" + file_path.rsplit(\".\", 1)[-1].lower()\n",
        "    if ext in LOADER_MAPPING:\n",
        "        loader_class, loader_args = LOADER_MAPPING[ext]\n",
        "        loader = loader_class(file_path, **loader_args)\n",
        "        return loader.load()\n",
        "\n",
        "    raise ValueError(f\"Unsupported file extension '{ext}'\")\n",
        "\n",
        "def load_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Loads all documents from the source documents directory, ignoring specified files\n",
        "    \"\"\"\n",
        "    all_files = []\n",
        "    for ext in LOADER_MAPPING:\n",
        "        all_files.extend(\n",
        "            glob.glob(os.path.join(source_dir, f\"**/*{ext.lower()}\"), recursive=True)\n",
        "        )\n",
        "        all_files.extend(\n",
        "            glob.glob(os.path.join(source_dir, f\"**/*{ext.upper()}\"), recursive=True)\n",
        "        )\n",
        "    filtered_files = [file_path for file_path in all_files if file_path not in ignored_files]\n",
        "\n",
        "    with Pool(processes=os.cpu_count()) as pool:\n",
        "        results = []\n",
        "        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\n",
        "            for i, docs in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\n",
        "                results.extend(docs)\n",
        "                pbar.update()\n",
        "\n",
        "    return results\n",
        "\n",
        "def process_documents(ignored_files: List[str] = []) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Load documents and split in chunks\n",
        "    \"\"\"\n",
        "    print(f\"Loading documents from {source_directory}\")\n",
        "    documents = load_documents(source_directory, ignored_files)\n",
        "    if not documents:\n",
        "        print(\"No new documents to load\")\n",
        "        exit(0)\n",
        "    print(f\"Loaded {len(documents)} new documents from {source_directory}\")\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    texts = text_splitter.split_documents(documents)\n",
        "    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n",
        "    return texts\n",
        "\n",
        "def does_vectorstore_exist(persist_directory: str, embeddings: HuggingFaceEmbeddings) -> bool:\n",
        "    \"\"\"\n",
        "    Checks if vectorstore exists\n",
        "    \"\"\"\n",
        "    db = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\n",
        "    if not db.get()['documents']:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def main():\n",
        "    # Create embeddings\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=embeddings_model_name)\n",
        "    # Chroma client\n",
        "    chroma_client = chromadb.PersistentClient(settings=CHROMA_SETTINGS , path=persist_directory)\n",
        "\n",
        "    if does_vectorstore_exist(persist_directory, embeddings):\n",
        "        # Update and store locally vectorstore\n",
        "        print(f\"Appending to existing vectorstore at {persist_directory}\")\n",
        "        db = Chroma(persist_directory=persist_directory, embedding_function=embeddings, client_settings=CHROMA_SETTINGS, client=chroma_client)\n",
        "        collection = db.get()\n",
        "        texts = process_documents([metadata['source'] for metadata in collection['metadatas']])\n",
        "        print(f\"Creating embeddings. May take some minutes...\")\n",
        "        db.add_documents(texts)\n",
        "    else:\n",
        "        # Create and store locally vectorstore\n",
        "        print(\"Creating new vectorstore\")\n",
        "        texts = process_documents()\n",
        "        print(f\"Creating embeddings. May take some minutes...\")\n",
        "        db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory, client_settings=CHROMA_SETTINGS, client=chroma_client)\n",
        "    db.persist()\n",
        "    db = None\n",
        "\n",
        "    print(f\"Ingestion complete! You can now run privateGPT.py to query your documents\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "2gPAhcKn9cW-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
